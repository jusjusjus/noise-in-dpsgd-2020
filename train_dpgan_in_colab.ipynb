{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jusjusjus/noise-in-dpsgd-2020/blob/master/train_dpgan_in_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Id-9FnHSn2Ex"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/jusjusjus/noise-in-dpsgd-2020.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zNb6bQtwR4nk"
   },
   "outputs": [],
   "source": [
    "cd noise-in-dpsgd-2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fciV-elcRsdh"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sVEk0h7SoD8I"
   },
   "outputs": [],
   "source": [
    "from os import makedirs\n",
    "from os.path import join, dirname\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from ganlib.dataset import Dataset\n",
    "\n",
    "from ganlib.classifier import Classifier\n",
    "\n",
    "torch.manual_seed(42 * 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sVEk0h7SoD8I"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    acc, examples_seen = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for i, (examples, labels) in enumerate(dataloader):\n",
    "            batch_size = labels.shape[0]\n",
    "            examples = examples.to(device)\n",
    "            labels = labels.to(device)\n",
    "            logits = model(examples)\n",
    "            y_pred = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            acc_i = (y_pred == labels).sum().item()\n",
    "            acc = (examples_seen * acc + acc_i) / (examples_seen + batch_size)\n",
    "            examples_seen += batch_size\n",
    "        \n",
    "    model.train()\n",
    "    return 100 * acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sVEk0h7SoD8I"
   },
   "outputs": [],
   "source": [
    "def schedule(lr, loss):\n",
    "    return lr if loss > 1.0 else loss * lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sVEk0h7SoD8I"
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 128\n",
    "lr_per_example = 1e-4\n",
    "eval_every = 1000\n",
    "adapt_every = 100\n",
    "weight_decay = 0.001\n",
    "best_model_filename = join(\"cache\", \"mnist_classifier.ckpt\")\n",
    "makedirs(dirname(best_model_filename), exist_ok=True)\n",
    "\n",
    "learning_rate = batch_size * lr_per_example\n",
    "\n",
    "print(f\"learning rate: {learning_rate} (at {batch_size}-minibatches)\")\n",
    "\n",
    "trainset = Dataset(labels=True, train=True)\n",
    "testset = Dataset(labels=True, train=False)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size,\n",
    "                         shuffle=True, num_workers=4)\n",
    "testloader = DataLoader(testset, batch_size=batch_size,\n",
    "                        shuffle=False, num_workers=4)\n",
    "\n",
    "clf = Classifier()\n",
    "clf = clf.cuda() if torch.cuda.is_available() else clf\n",
    "clf.train()\n",
    "device = next(clf.parameters()).device\n",
    "print(device)\n",
    "\n",
    "loss_op = nn.NLLLoss(reduction='mean')\n",
    "optimizer = optim.Adam(clf.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sVEk0h7SoD8I"
   },
   "outputs": [],
   "source": [
    "global_step, running_loss = 0, 1.0\n",
    "best_acc = 2.0\n",
    "for epoch in range(epochs):\n",
    "    for i, (examples, labels) in enumerate(trainloader):\n",
    "        batch_size = labels.shape[0]\n",
    "        examples = examples.to(device)\n",
    "        labels = labels.to(device)\n",
    "        logits = clf(examples)\n",
    "        loss = loss_op(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss = 0.99 * running_loss + 0.01 * loss.item()\n",
    "\n",
    "        if global_step % adapt_every == 0:\n",
    "            lr = schedule(learning_rate, running_loss)\n",
    "            print(f\"[{global_step}, epoch {epoch+1}] \"\n",
    "                  f\"train loss = {running_loss:.3f}, \"\n",
    "                  f\"new learning rate = {lr:.5f}\")\n",
    "            for g in optimizer.param_groups:\n",
    "                g.update(lr=lr)\n",
    "\n",
    "        if global_step % eval_every == 0:\n",
    "            acc = evaluate(clf, testloader)\n",
    "            print(f\"[{global_step}, epoch {epoch+1}] \"\n",
    "                  f\"train loss = {running_loss:.3f}, \"\n",
    "                  f\"test acc = {acc:.1f}\")\n",
    "\n",
    "            if acc > best_acc:\n",
    "                clf.to_checkpoint(best_model_filename)\n",
    "                best_acc = acc\n",
    "\n",
    "        global_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sVEk0h7SoD8I"
   },
   "outputs": [],
   "source": [
    "print(\"Running final evaluation\")\n",
    "acc = evaluate(clf, testloader)\n",
    "print(f\"[{global_step}, final evaluation] \"\n",
    "      f\"train loss = {running_loss:.3f}, \"\n",
    "      f\"test acc = {acc:.1f}\")\n",
    "\n",
    "if acc > best_acc:\n",
    "    clf.to_checkpoint(best_model_filename)\n",
    "    best_acc = acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "\n",
    "torch.manual_seed(42 * 42)\n",
    "\n",
    "from ganlib import scripts\n",
    "from ganlib.gan import GenerativeAdversarialNet\n",
    "from ganlib.logger import Logger\n",
    "from ganlib.privacy import compute_renyi_privacy\n",
    "from ganlib.trainer import DPWGANGPTrainer, WGANGPTrainer\n",
    "from ganlib.dataset import Dataset\n",
    "from ganlib.generator import MNISTGenerator, Optimizable\n",
    "\n",
    "cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTCritic(Optimizable):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        kw = {'padding': 2, 'stride': 2, 'kernel_size': 5}\n",
    "        C = capacity = 64\n",
    "        self.activation = nn.LeakyReLU(negative_slope=0.2)\n",
    "        self.conv1 = nn.Conv2d(1,     1 * C, **kw)\n",
    "        self.conv2 = nn.Conv2d(1 * C, 2 * C, **kw)\n",
    "        self.conv3 = nn.Conv2d(2 * C, 4 * C, **kw)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.projection = nn.Linear(4 * 4 * 4 * C, 1)\n",
    "\n",
    "    def forward(self, images):\n",
    "        images = self.activation(self.conv1(images))\n",
    "        images = self.activation(self.conv2(images))\n",
    "        images = self.activation(self.conv3(images))\n",
    "        images = self.flatten(images)\n",
    "        images = self.projection(images)\n",
    "        criticism =  images.squeeze(-1)\n",
    "        return criticism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(logger, info, tag, network, global_step):\n",
    "    \"\"\"print every 25, and plot every 250 steps network output\"\"\"\n",
    "    if global_step % 25 == 0:\n",
    "        logger.add_scalars(tag, info, global_step)\n",
    "        s = f\"[Step {global_step}] \"\n",
    "        s += ' '.join(f\"{tag}/{k} = {v:.3g}\" for k, v in info.items())\n",
    "        print(s)\n",
    "\n",
    "    if (global_step + 1) % 250 == 0:\n",
    "        ckpt = logger.add_checkpoint(network, global_step)\n",
    "        scripts.generate(logger=logger, params=ckpt,\n",
    "                         step=global_step)\n",
    "        if exists(join(\"cache\", \"mnist_classifier.ckpt\")):\n",
    "            scripts.inception(logger=logger, params=ckpt,\n",
    "                              step=global_step)\n",
    "        network.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set optional parameters\n",
    "#\n",
    "# nodp: set to true to train without differential privacy\n",
    "# sigma: noise multiplier determining epsilon of DP\n",
    "# grad_clip: per-example L2-gradient clipping constant\n",
    "#\n",
    "# Note: the log directory is adopted according to these parameters\n",
    "# so that you can view the algorithm output for different parameter\n",
    "# values side-by-side.\n",
    "\n",
    "nodp = False\n",
    "sigma = 0.5\n",
    "grad_clip = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some other default parameters\n",
    "\n",
    "batch_size = 128\n",
    "lr_per_example = 3.125e-6\n",
    "delta = 1e-5\n",
    "critic_steps = 4\n",
    "\n",
    "# Process parameters\n",
    "\n",
    "logdir = join('cache', 'logs')\n",
    "logdir = join(logdir, 'nodp' if nodp else f\"sigma_{sigma}-clip_{grad_clip}\")\n",
    "learning_rate = batch_size * lr_per_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize generator and critic.  We wrap generator and critic into\n",
    "# `GenerativeAdversarialNet` and provide methods `cuda` and `state_dict`\n",
    "\n",
    "generator = MNISTGenerator()\n",
    "critic = MNISTCritic()\n",
    "gan = GenerativeAdversarialNet(generator, critic)\n",
    "gan = gan.cuda() if cuda else gan\n",
    "\n",
    "dset = Dataset(labels=False, train=True)\n",
    "dataloader = DataLoader(dset, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=4)\n",
    "\n",
    "# Initialize optimization.  We make optimizers part of the network and provide\n",
    "# methods `.zero_grad` and `.step` to simplify the code.\n",
    "\n",
    "generator.init_optimizer(torch.optim.Adam, lr=learning_rate, betas=(0.5, 0.9))\n",
    "critic.init_optimizer(torch.optim.Adam, lr=learning_rate, betas=(0.5, 0.9))\n",
    "\n",
    "if nodp:\n",
    "    trainer = WGANGPTrainer(batch_size=batch_size)\n",
    "else:\n",
    "    print(\"training with differential privacy\")\n",
    "    print(f\"> delta = {delta}\")\n",
    "    print(f\"> sigma = {sigma}\")\n",
    "    print(f\"> L2-clip = {grad_clip}\")\n",
    "    trainer = DPWGANGPTrainer(sigma=sigma, l2_clip=grad_clip, batch_size=batch_size)\n",
    "\n",
    "print(f\"> learning rate = {learning_rate} (at {batch_size}-minibatches)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = {}\n",
    "global_step = 0\n",
    "logger = Logger(logdir=logdir)\n",
    "for epoch in range(100):\n",
    "    for imgs in dataloader:\n",
    "\n",
    "        if (global_step + 1) % critic_steps == 0:\n",
    "            genlog = trainer.generator_step(gan)\n",
    "            logs.update(**genlog)\n",
    "\n",
    "        critlog = trainer.critic_step(gan, imgs)\n",
    "        logs.update(**critlog)\n",
    "        \n",
    "        if not nodp:\n",
    "            spent = compute_renyi_privacy(\n",
    "                len(dset), batch_size, global_step + 1, sigma, delta)\n",
    "            logs['epsilon'] = spent.eps\n",
    "\n",
    "        log(logger, logs, 'train', gan, global_step)\n",
    "\n",
    "        global_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPvjfIHAlxWjRvpovkPmfGn",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "train-dpgan-in-colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
