{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "\n",
    "cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, num_batches=None):\n",
    "    \"\"\"Evaluate model on dataloader and return accuracy\"\"\"\n",
    "    model.eval()\n",
    "    acc = 0\n",
    "    with torch.no_grad():\n",
    "        for b, (images, labels) in enumerate(dataloader):\n",
    "            images = images.cuda() if cuda else images\n",
    "            labels = labels.cuda() if cuda else labels\n",
    "\n",
    "            logits = model(images)\n",
    "            predictions = logits.argmax(dim=1)\n",
    "            acc += (predictions == labels).float().mean()\n",
    "            if num_batches and b == num_batches:\n",
    "                break\n",
    "        return acc.item()/(b + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \"\"\"Return a new conv net\"\"\"\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(1, 16, 8, stride=2, padding=4),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Conv2d(16, 32, 4, stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(128, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, 10),\n",
    "        nn.LogSoftmax(dim=1)\n",
    "    )\n",
    "    return model.cuda() if cuda else model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the mnist train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trafo = Compose([\n",
    "    ToTensor()\n",
    "])\n",
    "trainset = MNIST('data/mnist', train=True, transform=trafo, download=True)\n",
    "print('train set size:', len(trainset))\n",
    "\n",
    "testset = MNIST('data/mnist', train=False, transform=trafo, download=True)\n",
    "print('test set size:', len(testset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train for `epochs` epochs and with minibatches of size `batch_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "batch_size = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset,\n",
    "    batch_size=512,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the parameters we set for the DP learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_norm_clip = 1.5\n",
    "noise_multiplier = 1.3\n",
    "learning_rate = 0.25\n",
    "delta = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# We make a copy of the gradients of all trainable parameters wherein we will\n",
    "# store clipped gradients of each example.\n",
    "\n",
    "gradients = {}\n",
    "for n, p in model.named_parameters():\n",
    "    gradients[n] = torch.zeros_like(p)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Starting epoch\", epoch)\n",
    "    running_loss = 0.0\n",
    "    for b, (images, labels) in enumerate(trainloader):\n",
    "        model.train()\n",
    "        images = images.cuda() if cuda else images\n",
    "        labels = labels.cuda() if cuda else labels\n",
    "\n",
    "        # Let's start by zeroing the gradients store.\n",
    "        \n",
    "        for _, grad in gradients.items():\n",
    "            grad.zero_()\n",
    "            \n",
    "        # Now we go image by image (microbatchsize = 1), and compute the\n",
    "        # gradients.  We clip the gradients, add noise and add these to\n",
    "        # the gradients store.\n",
    "        \n",
    "        batch_loss = 0.0\n",
    "        for image, label in zip(images, labels):\n",
    "            logit = model(image[None, ...])\n",
    "            loss = criterion(logit, label[None, ...])\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), l2_norm_clip, 2)\n",
    "            for name, param in model.named_parameters():\n",
    "                gradients[name] += param.grad\n",
    "                \n",
    "            batch_loss += loss.item()\n",
    "        \n",
    "        batch_loss /= images.shape[0]\n",
    "        running_loss = (b * running_loss + batch_loss) / (b+1) \n",
    "        \n",
    "        # Now we replace the model parameter gradients with the mean of all\n",
    "        # gradients we've gotten from individual images.  These are the gradients\n",
    "        # that the optimizer sees; so we can continue optimization by just stepping\n",
    "        # it up.\n",
    "        # Note that we add noise only in this step. This is because the independently\n",
    "        # sampled noise vectors per example add up to a scaled random perturbation.\n",
    "        \n",
    "        # Here we do it the wrong way to reproduce the results in\n",
    "        # \"github.com/tensorflow/privacy/tutorials/Classification_Privacy.ipynb\"\n",
    "        \n",
    "        sigma = l2_norm_clip * noise_multiplier / images.shape[0]\n",
    "\n",
    "        # Uncomment the following line to do it the right way:\n",
    "        \n",
    "        # sigma = l2_norm_clip * noise_multiplier / np.sqrt(images.shape[0])\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            noise = sigma * torch.randn_like(param.grad)\n",
    "            param.grad = gradients[name] / images.shape[0] + noise\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if b % 10 == 0:\n",
    "            acc = 100.0 * evaluate(model, testloader)\n",
    "            print(f\"[batch {b}, epoch {epoch}] train loss = {running_loss:.3f}, val acc = {acc:.1f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
    "\n",
    "compute_dp_sgd_privacy.compute_dp_sgd_privacy(\n",
    "    len(trainset), batch_size, noise_multiplier, epochs, delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we outline in the paper, `noise_multiplier` should be scaled by a factor `np.sqrt(batch_size)`, which gives us the following updated estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
    "\n",
    "compute_dp_sgd_privacy.compute_dp_sgd_privacy(\n",
    "    len(trainset), batch_size, noise_multiplier / np.sqrt(batch_size), epochs, delta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
